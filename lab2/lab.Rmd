### Uppgift 3.1.1

```{r }
  set.seed(4711)
  x1 <<- rgamma(n=10, shape = 4, scale = 1)
  x2 <<- rgamma(n=100, shape = 4, scale = 1)

```

## (1).
```{r }
  
  llgama <- function(x, alpha, beta){
    return((alpha - 1) * sum(log(x)) - (beta*sum(x)) +  length(x) * (alpha * log(beta) - lgamma(alpha)))
  }
  print(llgama(x1,2,2))
  

```
## (2).

```{r }
  alpha <- 4
  beta_values <- seq(0.01, 3, 0.01)
  #create an array
  y1 <- numeric(0)
  for(beta in beta_values){
    y1 <- c(y1, llgama(x1, alpha, beta))
  }
  y2 <- numeric(0)
  for(beta in beta_values){
    y2 <- c(y2, llgama(x2, alpha, beta))
  }
    plot(beta_values, y1)
    max1 <- beta_values[which.max(y1)]
    print(paste("Max värde =", max1))
    plot(beta_values, y2)
    max2 <- beta_values[which.max(y2)]
    print(paste("Max värde =", max2))
    

```

## (3).

```{r }
  alpha_values <- seq(0.01, 10, 0.01)
  beta <- 1
  #create an array
  y1 <- numeric(0)
  for(alpha in alpha_values){
    y1 <- c(y1, llgama(x1, alpha, beta))
  }
  y2 <- numeric(0)
  for(alpha in alpha_values){
    y2 <- c(y2, llgama(x2, alpha, beta))
  }
    plot(alpha_values, y1)
    max1 <- alpha_values[which.max(y1)]
    print(paste("Max värde =", max1))
    plot(alpha_values, y2)
    max2 <- alpha_values[which.max(y2)]
    print(paste("Max värde =", max2))


```
## (4 ).

```{r }
llnorm <- function(x,mu,sigma2){
  n <- length(x)
  return((-n/2)*log(2*pi*sigma2)+(-1/(2*sigma2)*sum((x-mu)^2)))
}
print(llnorm(x=x1,mu=2,sigma2=1))



```

## (5 ).

```{r }
sigma2 <- 1
mu_seq <- seq(0, 10, 0.01)
y1 <- numeric(0)
for ( mu in mu_seq) {
  y1 <- c(y1, llnorm(x1, mu, sigma2))
  
  }

y2 <- numeric(0)
for ( mu in mu_seq) {
  y2 <- c(y2, llnorm(x2, mu, sigma2))
  
  }

plot(mu_seq,y1)
plot(mu_seq,y2)

############################################################################################################################################################################
```
Genom att kolla på grafen ser första ut som en standardföredlning och den andra ser ut som en gammafördelning 

```{r}
    sigma2 <- 1
    mu_seq <- seq(0, 10, 0.01)
    y1 <- numeric(0)
    for (mu in mu_seq) {
        y1 <- c(y1, llnorm(x = x1, mu = mu, sigma2 = sigma2))
    }
    y2 <- numeric(0)
    for (mu in mu_seq) {
        y2 <- c(y2, llnorm(x = x2, mu = mu, sigma2 = sigma2))
    }
```
```{r, echo=FALSE}
    plot(mu_seq, y1)
    mu_max_x1 <- mu_seq[which.max(y1)]
    print(paste("max value at alpha =", mu_max_x1))
    plot(mu_seq, y2)
    mu_max_x2 <- mu_seq[which.max(y2)]
    print(paste("max value at alpha =", mu_max_x2))
```
```{r}
    y1 <- dgamma(x1, shape = alpha_max_x1, scale = beta_max_x1)
    hist(y1)
    y2 <- dgamma(x2, shape = alpha_max_x2, scale = beta_max_x2)
    hist(y2)

    y3 <- dnorm(x1, mean = mu_max_x1, sd = 1)
    hist(y3)
    y4 <- dnorm(x2, mean = mu_max_x2, sd = 1)
    hist(y4)

    hist(x1)
    hist(x2)
```

## 3.2.1
```{r }
gamma_beta_mle <- function(x,alpha){
  n<-length(x)
  a<-alpha
  coolSum <- n*a*(sum(unlist(x)))^-1;
  

  return(coolSum)
}
print(gamma_beta_mle(x1,2))

```
För x1 så maximeras chansen för att få dessa värden då beta är 0.768
För x2 så maximeras chansen för att få dessa värden då beta är 0.961

```{r }
#1
test_x <- 1:10
norm_mu_mle <- function(x){
  n<-length(x)
  coolSum <- (1/n)*sum(unlist(x));
  return(coolSum)
}


norm_sigma2_mle <- function(x){
  xMean <- mean(unlist(x))
  coolSum = 0
  for(i in x){
      
      coolSum = coolSum + ((i-xMean)^2)
  }
  return((1/length(x))*coolSum)

  }


print(norm_mu_mle(test_x))
print(norm_sigma2_mle(test_x))

```

```{r }
#2 
set.seed(42)
y10 <- rnorm(10,10,2)
y10000 <- rnorm(10000,10,2)

print("Uppgift 2")
print(norm_mu_mle(y10))
print(norm_sigma2_mle(y10))
print(norm_mu_mle(y10000))
print(norm_sigma2_mle(y10000))

#Visar att fler dragningar i norm_mu_mle(x) gör att värdet närmar sig medelvärdet/variansen
```

##3.3.1
###1
```{r }
  log_likelihood_beta <- function(params, data) {
    alpha <- params[1]
    beta <- params[2]
    log_likelihood <- sum(dbeta(data, shape1 = alpha, shape2 = beta, log = TRUE))
    return(-log_likelihood)  # Returnera negativt värde eftersom optim() minimiserar
  }


```
###2
```{r }
  simulated_data <- rbeta(100, shape1 = 0.2, shape2 = 2)
  hist(simulated_data)
```
###3
```{r }
    opt_res <- optim(par = c(1, 1), fn = log_likelihood_beta, x = simulated_data, method = "L-BFGS-B", lower = 0.000001)
    print(opt_res$par)
```

##3.4.1
###1
```{r }
    n <- 2000
    beta1 <- numeric(n)
    beta2 <- numeric(n)
    m1 <- numeric(n)
    m2 <- numeric(n)
    sigma1 <- numeric(n)
    sigma2 <- numeric(n)

    for (i in 1:n) {
      
      
        y1 <- rnorm(n = 10, mean = 10, sd = 2)
        y2 <- rnorm(n = 10000, mean = 10, sd = 2)
        m1[i] <- norm_mu_mle(x = y1)
        m2[i] <- norm_mu_mle(x = y2)
        sigma1[i] <- norm_sigma2_mle(x = y1)
        sigma2[i] <- norm_sigma2_mle(x = y2)
        x1 <- rgamma(n = 10, shape = 4, rate = 1)
        x2 <- rgamma(n = 10000, shape = 4, rate = 1)
        beta1[i] <- gamma_beta_mle(x = x1, alpha = 4)
        beta2[i] <- gamma_beta_mle(x = x2, alpha = 4)

        
    }
    
    hist(beta1)
    hist(beta2)
    hist(sigma1)
    hist(sigma2)
    hist(m1)
    hist(m2)
    
```
Vår slutsats är att när vi höjer värdet på n så minskar variansen.
###2
```{r}
    n <- 2000
    
    m1 <- numeric(n)
    m2 <- numeric(n)
    beta1 <- numeric(n)
    beta2 <- numeric(n)
    sigma1 <- numeric(n)
    sigma2 <- numeric(n)
    x1 <- rgamma(n = 10, shape = 4, rate = 1)
    x2 <- rgamma(n = 10000, shape = 4, rate = 1)
    y1 <- rnorm(n = 10, mean = 10, sd = 2)
    y2 <- rnorm(n = 10000, mean = 10, sd = 2)

  for (i in 1:n) {
      sigma1[i] <- norm_sigma2_mle(x = sample(y1, 10, replace = TRUE))
        sigma2[i] <- norm_sigma2_mle(x = sample(y2, 10000, replace = TRUE))
        beta1[i] <- gamma_beta_mle(x = sample(x1, 10, replace = TRUE), alpha = 4)
        beta2[i] <- gamma_beta_mle(x = sample(x2, 10000, replace = TRUE), alpha = 4)
        m1[i] <- norm_mu_mle(x = sample(y1, 10, replace = TRUE))
        m2[i] <- norm_mu_mle(x = sample(y2, 10000, replace = TRUE))
        
    }
    hist(beta1)
    hist(beta2)
    hist(sigma1)
  hist(sigma2)
    hist(m1)
    hist(m2)
   
```
När vi inte längre använder oss a populationen utan istället kör på stickprov så kommer vår data ej bli lika exakt.